\section{Experiments}\label{s:experiments}
We trained the network for about two weeks, looking for the perfect set of parameters. However, two days before delivery we found a bug that invalidated all our results. We were instantiating the training, validation and test set with the same python method with a parameter, that indicated what kind of dataset we needed. The training and validation accuracy were coherent and was going up and down as we changed the hyperparameters, but the fine-tuning task was not giving good results. In addition, the evaluation of the network was random: once it was coherent with the training and validation accuracy, another time was completely different. We debug for many days until we found that probably the computational graph wasn't built too well in case of shared weights (we had not found any literature about this). So, we created three distinct (and mostly equal) functions that retrieve the training, validation and test sets: this structure let us train and evaluate the network. However, the time was too little to train the network profitably.\newline
The experimental part is made up of two different parts even if the fine-tuning part depends mostly on the results of the pretext task. 

% ------------------------------------------------------------------------------------------------ %

\subsection{Jigsaw puzzle task performance}
As seen in \ref{sss:PC_arch} and \ref{sss:GCP_arch} we used two different network architectures to train the Jigsaw puzzle that served two different purposes:
\begin{itemize}
    \item the PC network to implement the paper pretext task \emph{as-it-is};
    \item the GCP network to try to outperform previous results in performances and execution time.
\end{itemize}
The training tactics have been slightly different, so they will be presented in two separate sections.

\subsubsection{The PC network training}\label{sss:PC_training}
We trained the PC network making use of those assumptions:
\begin{itemize}
    \item \textbf{tunable hyperparameters}: the tunable parameters were the batch size, the learning rate, its decay rate and the momentum of SGD optimizer;
    \item \textbf{hyperparameters search}: since the GPU RAM was not so big we set the batch size as high as possible to fit the entire GPU. In this way we set it to 100 both for training and validation. As regard the learning rate, its decay rate and the momentum we tried different combination until we got a good result. We read many articles to understand the best way to tune those parameters until one \cite{SGD_momentum} gave us the theory and the \textit{rule-of-thumb}: the more the momentum, the less the learning rate;
    \item \textbf{stopping policy}: we used the early stopping policy to stop the training whenever the validation loss started to increase. In this way we let the personal computer to run the session only until it was useful and to stop as soon as possible.
\end{itemize}

The best performance that we gained is:
\begin{table}[!ht]
    \begin{center}
        \label{tab:PC_early_results}
        \rowcolors{2}{gray!25}{white}
        \begin{tabular}{l|l}
            \rowcolor{gray!50}
            \textbf{Type} & \textbf{Value} \\
            \hline
            Training accuracy & 0.457\\
            Training loss & 2.081\\
            Validation accuracy & 0.454\\
            Validation loss & 1.915\\
            Training time & 3h33min\\
            Learning rate & 1e-3\\
            Learning rate decay & 0.97\\
            SGD Momentum & 0\\
            \hline
        \end{tabular}
    \end{center}
    \caption{Results for PC network}
\end{table}
We can see from fig. \ref{fig:early_PC_acc} and \ref{fig:early_PC_loss} that the results are promising, although the network started overfitting as soon as it reached the 0.45\% accuracy on validation set.

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.60]{images/PC_early_accuracy.png}
    \caption{Accuracy results of PC network}
    \label{fig:early_PC_acc}
\end{figure}
\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.60]{images/PC_early_loss.png}
    \caption{Loss results of PC network}
    \label{fig:early_PC_loss}
\end{figure}

\subsubsection{The GCP network training}\label{sss:GCP_training}
The training of the GCP network has been more difficult since we were free to add and remove some other layers in the network. The best configuration we found before the bug was the one explained in \ref{sss:GCP_arch} and gave very good results. However, we could not trust those results and we started the training from scratch. We found that the Adam optimizer was more difficult to train since its trend is more sensible to the learning rate and tend to overfit easily. We controlled that overfit with many batch normalization layers, that stabilized the weights between convolutions and before the output layers. With this in mind, we used those assumption to train the network:
\begin{itemize}
    \item \textbf{tunable hyperparameters}: the tunable parameters were the batch size, the learning rate, its decay rate and the momentum of the batch normalization;
    \item \textbf{position of batch norm layers}: we tried different combination to see in which position the batch normalization layers were more effective in stabilizing the training. We have seen so far that they help more before the max pool layers of the AlexNet block and before the last dropout, near the output;
    \item \textbf{hyperparameters search}: to speed up learning without loss in generalization we set the batch size to 512, as high as we could. As regard the learning rate, we use it to slow down learning as much as possible, since the Adam optimizer had the tendency to converge rapidly (and to overfit). We tuned the learning rate decay with the same purpose. The batch normalization momentum was the trickiest hyperparameter to be tuned: too high carried no learning, too low overfit. We took inspiration from various online articles (see for example \cite{batch_norm_online}), however none of them helped us very much. We finally found that \emph{very} high momentum values were helping us to learn without overfit. In addition, we added a L2 regularization for weights and bias of the last AlexNet dense layer: this helped us to slow down learning and stabilize the loss function \cite{l2_neural_networks}. Therefore, the loss values are higher than the PC counterpart;
    \item \textbf{stopping policy}: as the online virtual instance of the machine is paid per hour and not for use we only set a maximum value of epochs. In case of overfitting we stopped the training as soon as we saw it.
\end{itemize}
The best performance that we gained is:
\begin{table}[!ht]
    \begin{center}
        \label{tab:GCP_early_results}
        \rowcolors{2}{gray!25}{white}
        \begin{tabular}{l|l}
            \rowcolor{gray!50}
            \textbf{Type} & \textbf{Value} \\
            \hline
            Training accuracy & 0.398\\
            Training loss & 4.898\\
            Validation accuracy & 0.367\\
            Validation loss & 5.202\\
            Training time & 6h6min\\
            Learning rate & 5e-6\\
            Learning rate decay & 0.80\\
            BN Momentum & 0.99 AlexNet, 0.997 last\\
            \hline
        \end{tabular}
    \end{center}
    \caption{Results for GCP network}
\end{table}
We can see from fig. \ref{fig:early_GCP_acc} and \ref{fig:early_GCP_loss} that the results are promising, although the network started overfitting as soon as it reached the 0.36\% accuracy on validation set.

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.60]{images/GCP_early_accuracy.png}
    \caption{Early accuracy results of GCP network}
    \label{fig:early_GCP_acc}
\end{figure}
\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.60]{images/GCP_early_loss.png}
    \caption{Early loss results of GCP network}
    \label{fig:early_GCP_loss}
\end{figure}

% ------------------------------------------------------------------------------------------------ %

\subsection{Fine-tuning on Food Dataset}
As we have already explained, the little time we had to train the JPS network - and its consequently poor performance - did not let us have great results also in the fine-tuning task. Indeed, the results are really bad.\newline
We will discuss in two different sections the results that we have obtained from the weights of the PC and the GCP networks. We have trained this network only with the personal computer since the free credits of GCP were exhausted.

\subsubsection{Assumptions of the training}\label{sss:TL_training}
We trained the target task making use of those assumptions:
\begin{itemize}
    \item \textbf{tunable hyperparameters}: the tunable parameters were the batch size and the learning rate;
    \item \textbf{hyperparameters search}: the batch size and the learning rate are bounded: the more the first, the more the latter. However the batch size that was fitting the personal computer was not so much, so we kept the learning rate very low;
    \item \textbf{stopping policy}: we used the early stopping policy to stop the training whenever the validation loss started to increase. In this way we let the personal computer to run the session only until it was useful and to stop as soon as possible.
\end{itemize}

\subsection{Performance coming from the PC network}
The best performance that we gained is:
\begin{table}[!ht]
    \begin{center}
        \label{tab:TL_PC_early_results}
        \rowcolors{2}{gray!25}{white}
        \begin{tabular}{l|l}
            \rowcolor{gray!50}
            \textbf{Type} & \textbf{Value} \\
            \hline
            Training accuracy & 0.0221\\
            Training loss & 5.087\\
            Validation accuracy & 0.014\\
            Validation loss & 5.383\\
            Training time & 30min\\
            Learning rate & 1e-5\\
            Learning rate decay & 0.97\\
            Batch size & 50\\
            \hline
        \end{tabular}
    \end{center}
    \caption{Results for the transfer learning task coming from the PC network}
\end{table}
We can see from fig. \ref{fig:TL_PC_acc} and \ref{fig:TL_PC_loss} that even if the time spent for the training is little, the results are far from promising.

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.60]{images/TL_PC_acc.png}
    \caption{Accuracy results of the transfer learning task coming from the PC network}
    \label{fig:TL_PC_acc}
\end{figure}
\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.60]{images/TL_PC_loss.png}
    \caption{Loss results of the transfer learning task coming from the PC network}
    \label{fig:TL_PC_loss}
\end{figure}

\subsubsection{Performance coming from the GCP network}\label{sss:GCP_training}
The best performance that we gained is:
\begin{table}[!ht]
    \begin{center}
        \label{tab:GCP_early_results}
        \rowcolors{2}{gray!25}{white}
        \begin{tabular}{l|l}
            \rowcolor{gray!50}
            \textbf{Type} & \textbf{Value} \\
            \hline
            Training accuracy & 0.0423\\
            Training loss & 5.083\\
            Validation accuracy & 0.025\\
            Validation loss & 6.789\\
            Training time & 23min\\
            Learning rate & 1e-6\\
            Learning rate decay & 0.97\\
            Batch size & 50\\
            \hline
        \end{tabular}
    \end{center}
    \caption{Results for GCP network}
\end{table}
We can see from fig. \ref{fig:TL_GCP_acc} and \ref{fig:TL_GCP_loss} that even if the results are better than the one that comes from the PC network, they are far from satisfying.
\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.60]{images/TL_GCP_acc.png}
    \caption{Accuracy results of the transfer learning task coming from the GCP network}
    \label{fig:TL_GCP_acc}
\end{figure}
\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.60]{images/TL_GCP_loss.png}
    \caption{Accuracy results of the transfer learning task coming from the GCP network}
    \label{fig:TL_GCP_loss}
\end{figure}

% ------------------------------------------------------------------------------------------------ %

\subsection{Machines details}\label{ss:machines}
The personal computer with which we have trained the pre-task neural network is a laptop with:
\begin{itemize}
    \item CPU: Intel Core i5 8300H (2.30GHz, 3.90GHz Turbo);
    \item GPU: NVidia GTX 1050 (4GB RAM);
    \item Memory: 16GB DDR4 2666MHz;
    \item Persistency: SSD Samsung 970 evo NVMe;
    \item O.S.: Windows 10 Home.
\end{itemize}
The virtual machine that we have instantiated is composed of:
\begin{itemize}
    \item vCPU: 4-core;
    \item GPU: NVidia Tesla T4 (12GB RAM);
    \item vMemory: 7.8GB
    \item Persistency: SSD 65GB;
    \item O.S.: Ubuntu 18.10.
\end{itemize}



